Kubernetes Study

Saturday 14 Dec 2024

KUBE-API SERVER:
When we run any command, its authenticated the request and validated first.
Eg: Creating a pod.
1. Kubeapi server authenticated the request and validated.
2. It creates pod object without assiging a node.
3. Updates the information in etcd server and updates the user that pod has been created.
4. Scheduler continuously monitor kube api server and realised there is new pod with no node assigned.
5. Scheduler will find new node for the pod and communicates back to the kube api server.
6. APi server updates the information in the etcd cluster.
7. API server then passes that info to the kubelet of appropriate worker node.
8. Kubetlet then creates a pod on the node and instruct the container runtime engine to deploy the app image
9. Once done kubelet updates the status back to the kube api server
10. The kube api server updates the data back to the etcd cluster.

Silimar pattern follows for the most of the request. 


Tasks: Download kube api server binary and run as a service in kubernetes master node.

When we setup cluster using kubeadm tool, kube-apiserver deploy as a pod in kube-system namespace in master node.
Manifests present in /etc/kubernetes/manifests/ folder.

[root@master manifests]# cat kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.0.10:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.0.10
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.29.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.168.0.10
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.168.0.10
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.168.0.10
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
status: {}
[root@master manifests]#


--

[root@master manifests]# cat etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.0.10:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.0.10:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.168.0.10:2380
    - --initial-cluster=master=https://192.168.0.10:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.0.10:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.168.0.10:2380
    - --name=master
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.12-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
[root@master manifests]#

--




In non kubeadm installation we can inspect the kubeapi server using inspecting service:
cat /etc/systemd/system/kube-apiserver.service

We can also confirm whether apiserver is running or not using:
ps -ef | grep kube-apiserver

[root@master manifests]# ps -ef | grep kube-apiserver
root        1493       1  0 05:15 ?        00:00:00 /usr/bin/conmon -b /run/containers/storage/overlay-containers/0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0/userdata -c 0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0 --exit-dir /var/run/crio/exits -l /var/log/pods/kube-system_kube-apiserver-master_cb810db0f8d019db342aabd393495217/kube-apiserver/4.log --log-level info -n k8s_kube-apiserver_kube-apiserver-master_kube-system_cb810db0f8d019db342aabd393495217_4 -P /run/containers/storage/overlay-containers/0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0/userdata/conmon-pidfile -p /run/containers/storage/overlay-containers/0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0/userdata/pidfile --persist-dir /var/lib/containers/storage/overlay-containers/0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0/userdata -r /usr/bin/runc --runtime-arg --root=/run/runc --socket-dir-path /var/run/crio --syslog -u 0344b1bb1448eac235572ffac1f9eec8360f6ff417b9fdcc1df095d7312195f0 -s
root        1522    1493  2 05:15 ?        00:08:15 kube-apiserver --advertise-address=192.168.0.10 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root       66254   61617  0 09:53 pts/0    00:00:00 grep --color=auto kube-apiserver

===================================

KUBE-CONTROLLER MANAGER:
Controllers are processes which are conitnuously monitoring state of cluster and works towards bringing the whole system at desired functioning state.
For eg node controller monitors the node with the help of kubeapi server. Node controller check the status of the nodes every 5sec (Node Monitor period). 
If it does not received response from the node continuosluy in 40 sec (Node Monitor grace period) so it marks node as a unreachable.
After unreachable it gives chance to node for 5min (Pod eviction timeout) to come back up. If it does not it removes the pods assign to that node and provisions them in healthy one if the pod are part of replicaset.

Replication Controller:
Responsible for monitoring status of replicaset and insuring that desired number of pods always available within the set. 
If pod dies it creates another one.
It is good for high availability.
Can we have replication controller for single pod?
Ans: Yes it is good idea to have it, so when pod dies it will automatically creates new one.

Replication controller span across multiple nodes in the cluster.

Difference between Replication controller vs Replica Set.
=> Both have same purpose.
  Replication controller is the older technology which replace by Replica Set.
  Replicaset requires a selector definition, it will tell replicaset what pods fall under it. This is major difference.
  Why it need to know the pods falls under it?
  => Because Replicaset also manages pod that were not created as part of Replica Set creation.
     For eg: If there are pods which are created before replicaset that match labels specified in the selector, 
	          the replicaset will also take these pods into consideration when creating the replicas.
   
 
How to scale replicaset?
Ans: Many ways - 
	kubectl replace -f replicaset.yaml
	kubectl scale --replicas=6 -f replicaset.yaml   (It will not update the file)
	kubectl scale --replicas=6 replicaset myapp-replicaset

We can also automatically scale replicas based on nodes.


Suppose I have mentioned incorrect image in replicaset definition file and mycurrent pod is in ImageErrPull state.
And i update image name in manifest file and apply changes using command - 
 # kubectl replace -f new-replicaset.yaml 
But still pods showing imageerrpull issue, to resolve this we need first delete all issues pods, so that replicaset
will create new pod with new images.

In replicaset: Selector labels and pod template labels should always match, otherwise you will get an error.


Some more controllers example:
1. Node Controller
2. Replication controller
3. Deployment CONTROLLER
4. Namespace controller 
5. Endpoint controller 
6. Service Account Controller 
7. Job Controller 
8. PV protection controller 
9. PV binder controller 
10. Cronjob 
11. Stateful Set 
12. Replicaset 

How to see all these controllers and where they are located?
=> All of them packages into kube-controller-manager 	



Task: Install kube-controller-manager binary and install as a service.

If we setup cluster using kubeadm, it runs as a pod in kube-system namespace and manifests present in - 
/etc/kubernetes/manifests/


[root@master manifests]# cat kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.29.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
status: {}
[root@master manifests]#

----

If you are running as a service then check - 
cat /etc/systemd/system/kube-controller-manager.service

ps -ef | grep kube-controller-manager

[root@master manifests]# ps -ef | grep kube-controller-manager
root        1492       1  0 05:15 ?        00:00:00 /usr/bin/conmon -b /run/containers/storage/overlay-containers/3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e/userdata -c 3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e --exit-dir /var/run/crio/exits -l /var/log/pods/kube-system_kube-controller-manager-master_6a327d6e651ba50513a24327502fb289kube-controller-manager/5.log --log-level info -n k8s_kube-controller-manager_kube-controller-manager-master_kube-system_6a327d6e651ba50513a24327502fb289_5 -P /run/containers/storage/overlay-containers/3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e/userdata/conmon-pidfile -p /run/containers/storage/overlay-containers/3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e/userdata/pidfile --persist-dir /var/lib/containers/storage/overlay-containers/3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e/userdata -r /usr/bin/runc --runtime-arg --root=/run/runc --socket-dir-path /var/run/crio --syslog -u 3d3c3e7610e533e3f0a01542e3be74d68013813cce3fd5956ee4a57942b9747e -s
root        1528    1492  0 05:15 ?        00:02:07 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true
root       69802   61617  0 10:09 pts/0    00:00:00 grep --color=auto kube-controller-manager
[root@master manifests]#

============================

KUBE-SCHEDULER 
kube-scheduler only decides which pod goes on which node.
It does not actually place the pod on the node, that is job of kubelet. Kubelet creates the pod in the node.

Scheduler finds best node for pod based on its requirement.
For eg: Pod requirement is 10 CPU 
1. kube-scheudler filter out the nodes (not sufficient cpu) which do not fit in the requirement.
2. Lets say it filter two nodes and two are reamining. Then scheduler rank the nodes for best fit for the pod.
   It uses priority function to assign a score to node on scale 0 to 10.
   Eg: Scheduler calculates the amount of resources that would be free on the nodes after placing the pod on them and that node get the better rank and it wins.
3. It also checks for resource requirements and limits, taints and tolerations, node selectors/affinity.

Scheduler can be customize and we can write our own scheduler as well.

Taks: Donwload kube-scheduler binary and run as a service.

If we install using kubeadm tool, then it run as a pod in kube-system namespace.



[root@master manifests]# cat kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    image: registry.k8s.io/kube-scheduler:v1.29.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}
[root@master manifests]#

--

ps -ef | grep kube-scheduler
[root@master manifests]# ps -ef | grep kube-scheduler
root        1497       1  0 05:15 ?        00:00:00 /usr/bin/conmon -b /run/containers/storage/overlay-containers/2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6/userdata -c 2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6 --exit-dir /var/run/crio/exits -l /var/log/pods/kube-system_kube-scheduler-master_ef28d874ec37e9e2f263d690fdf1b1dd/kube-scheduler/5.log --log-level info -n k8s_kube-scheduler_kube-scheduler-master_kube-system_ef28d874ec37e9e2f263d690fdf1b1dd_5 -P /run/containers/storage/overlay-containers/2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6/userdata/conmon-pidfile -p /run/containers/storage/overlay-containers/2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6/userdata/pidfile --persist-dir /var/lib/containers/storage/overlay-containers/2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6/userdata -r /usr/bin/runc --runtime-arg --root=/run/runc --socket-dir-path /var/run/crio --syslog -u 2eaef62d9be8833ca5a14a0c6ee1cdc2e47fa9d6b7842569251d5b26df0c3ff6 -s
root        1542    1497  0 05:15 ?        00:00:32 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true
root       72187   61617  0 10:20 pts/0    00:00:00 grep --color=auto kube-scheduler

==========================================

KUBELET:
It registers a node with kubernetes cluster.
When it recives the instruction to run pod on the node, it requests the container runtime engine to pull required image and run an instance.
The kubelet continues to monitor state of the pod and its containers and reports to the kubeapi server on timely basis.

If we use kubeadm tool to deploy cluster, it does not automatically deploy the kubelets.

You must always manually install the kubelet on worker node.
Download and extract and run it as a service.

ps -ef | grep kubelet

[root@master manifests]# ps -ef | grep kubelet
root        1277       1  1 05:15 ?        00:04:27 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/crio/crio.sock --pod-infra-container-image=registry.k8s.io/pause:3.9
root        1522    1493  2 05:15 ?        00:09:07 kube-apiserver --advertise-address=192.168.0.10 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root        3609    3599  0 05:15 ?        00:00:00 /usr/local/bin/node-driver-registrar --v=5 --csi-address=/csi/csi.sock --kubelet-registration-path=/var/lib/kubelet/plugins/csi.tigera.io/csi.sock
root       73880   61617  0 10:27 pts/0    00:00:00 grep --color=auto kubelet

==========================================

KUBE-PROXY:

By default pod can communiacted with another pod in the cluster using pod network. There are multiple pod networking solutions available - calico, etc.

Service is virtual component which is in kubernetes memory.
How service is accessible in the cluster across the node?
=> using kube-proxy 
   Kube-proxy is a process which runs on cluster nodes and its job is to look for new services, and everytime when service is created, kube-proxy creates 
    a appropriate rule on each node to forward traffic to those services to the backend pods.
	One way it does this using iptables rules.
	
Taks: Download kube-proxy binary, extract it and run it as a service.

Kubeadm tool run kube-proxy as a pod on each node by running it as a daemonset so single pod alwasys deployed in a cluster.

kubectl get daemonset -n kube-system 

==========================================

When to use multi-container pod?
Ans: If you need some helper container with our app/web container for processing user data, files, etc. 
You can have multi-containers in a pod.So when app container dies, helper also dies and when app container deploy, helper also deploy.
In multi-container pod, all containers can reach out to each other using localhost as they shared same network space.
Also they can easily share same storage space as well.
==========================================


Pod definition files: 
Always contains -
	apiVersion:
	kind:
	metadata:

	spec:


apiVersion - API Version of object we are trying to create
  Pod - v1
  Service - v1
  ReplicaSet - apps/v1
  Deployment- apps/v1 
  
kind - Type of object we are trying to create  

metadata - Data about the object like its name, labels, etc and it is in the form of dictionary

   name: - string value 
   labels  - dictionary inside the metadata dictionary which has key value pairs
       app: myapp

 
spec - It is a dictionary 
  containers:     - list/array
     - name: nginx-controller    (1st item of the disk)
	   image: nginx 
     

kubectl create -f pod-definition.yml 



--
# kubectl get pods
Status can be - Running, ImagePullBackOff

Ready column indicates - Number of containers running/ Total number of containers

controlplane ~ ✖ kubectl apply -f redis.yml 
Warning: resource pods/redis is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
pod/redis configured

----

--dry-run is deprecated so replace it with --dry-run=client 

Use dry-run to generate yaml file - 
# kubectl run redis --image=redis --dry-run=client -o yaml > redis.yml  

-----


Tip: If you are unsure about the apiVersion always run command - # kubectl explain replicaset ,
       here group: apps and version: v1 so final apiVersion would be apps/v1
	   

-==================================

DEPLOYMENTS:

Rolling updates: When you have multiple pods of application running and you wanted to do upgrade, then rolling uppdate do one by one.


Deployment advantages:
1. Rolling updates
2. Easy rollback changes
3. 


Deployment provides capabilities to upgrade underline instances seamlessly using rolling updates, undo changes and pause and resume changes as requireed.

Deployment definition exactly same as replicaset definition except kind now as 'Deployment'.

Deployment automatically creates replicaset.

Deployments --creates--> Replicaset --creates--> Pods

-----------

kuebctl get all -- shows pods, services, replicaset, deployments


Refer for commands:
https://kubernetes.io/docs/reference/kubectl/conventions/

==========================

SERVICES:
1. NodePort
2. ClusterIP
3. LoadBalancer

Nodeport --> When we want to access application/website from external, we can use nodeport service.
			 Service will map port on the node to a port on the pod
			 There are 3 ports invole - port of pod (target port) where actual webservice running for eg: 80 
										port of the service (port) eg: 80
										port of the node (nodeport) which we use to access app externally. eg: 30008
			 Service itself has its own IP address, that is called cluster IP of the service.	
			 Nodeport can only be in valid range which by default of 30000 - 32767.
			 Mandatary field is - port (port of the service), if we do not provide target port by default it assume port's port and 
									if we do not provide nodePort then it takes free port from valid 30000-32767 range
			 ports is an array (-), we can add multiple port mapping in single service.

			 Service act as a built in load balancer to disribute the load and It uses random algorithm to balance traffic between the pod and session affinity is set to yes.
			 
			 Service spans across the nodes. So even if particular does not have pod running but in future it comes, you don't need to do any additional thing, service will take care of that pod if it has required labels.
			 Even your pod is deployed on two nodes it will still accessible through the IP of all the nodes of the cluster.
How to connect the service to the pod?
Ans: Using selectors and labels (no need to add matchLabels keyword here, we can directly use labels).


ClusterIP --> 
   It groups the pods and provide single endpoint to reach out to the pods. So even pods are recreated and its IP changes, frontend pod can reqach out to backend pods without an issue.
   
   
LoadBalancer --> For load balancer service - Create deployments, create clusterIp service and then create LoadBalancer service.
				NodePort is useful to access application externally, but what URL you can provide to the end user.
				As Nodeport is not practical approach.
				For on-prem we need to deploy one VM with laod balancer like HA-Proxy or nginx and loadbalancer on the node but this is tedious task.
				It is easy if we use cloud like GCP, AWS, etc.


When I describe service I see the endpoints what are these? What happened if we accidently assign same labels to any different pod which service created doesn't service route traffic to that pod as well?
Ans: Here endpoints comes into picture, so when we create service we are thinking as we are giving slectors and labels service routes traffic to the pod on the basis of labels.
     But it is not like that, when we create service based on labels service identifies pod and its IP address and port mark as endpoints.
     Hence you can see same number of endpoints which pod matches while creation of the service.
     So even if you accidently creates new pod with same label, service will not route traffic there.

When you creates service and bymistake given wrong label and your service is not routing traffic so always use #kubectl describe svc command and check for if any endpoints map or not, if it is zero (0), then check for labels, it is best way to troubleshoot this issue.

Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
	 (This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
(This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



==========================

NAMESPACES:
   Namespace is a isolated environment for kubernetes objects.
   By default kubernetes creates 'default' namespace for us.
   Kubernetes creates set of pods and services for its internal purpose in kube-system namespace. It creates in different namespace to save resources by accidently deletion from the user.
   Kuebrnetes creates 3rd namespace automatically 'kube-public'. Here resources which should be available to all users are created.
   We can create custom namespaces using # kubectl create ns command for different applications as per our need to isolcate from other application.
   Each namespace has its own set of policies that defines who can do what.
   We can also assign quota to each of these name spaces, in that way each namespace is guaranteed a certain amount and does not use more than its limit.
   The resources in each namespaces can refer their resources simply by their names.
      eg: mysql.connect("db-service") 
   If pod of one namespace wants to access pod of another namespace, we need to provide namespace name as well.
      syntax - servicename.namespace.svc.cluster.local  we need to add this because when the service is created, a DNS entry add in this format.
	      cluster.local --> Is a default domain of the kubernetes cluster 
		  svc --> subdomain for service
      eg: mysql.connect("db-service.dev.svc.cluster.local") where dev is the namespace name.
   In the definition yaml files we can add namespace in the metadata section metadata.namespace
   
   Que. How to switch dev namespace permanently, so we dont want to specify namespace option with command?
   Ans: kubectl config set-context $(kubectl config current-context) --namespace=dev
   
Resource Quota:
    We can restrict the namespace, so that it does not consume all resources of the cluster.
	We can check whether quota exceededs of particular namespace or not using -
		# kubectl get resourcequota -n dev
		# kubectl describe ns dev

==========================

IMPERATIVE VS DECLARATIVE APPROACH

Imperative Approach - Step by step detailed instrucion 

Declarative - Just add requirements 
              Eg: Terraform, Puppet, Chef, Ansible
			  
In kubernetes world, imperative approach is managing objects using commands to create, update and delete objects.
                     This approach is quick as we do not need to play with yaml files.
					 It is hard to keep track of how these objects are created.
					 Better way is update the configuration in yaml file and use # kubectl replace -f nginx.yaml command to apply changes.
                     To completely delete and deploy object again use # kubectl replace --force -f nginx.yaml command. This is still imperative appraoch because you are still using replace and delete commands.
					 You should aware that object is exist before you replace that.
					 Create comamnd will fail if object is already exist.
					 As a system administartor imperative approach is very taxing as you must always be aware of current configurations and perform checks to make sure things are in place before making the change.
					 
Declarative approach is create set of files with instruction and run single kubectl apply command to deploy resources.
      # kubectl apply -f nginx.yaml => It look for the existing configuration and figure out what changes needs to be done to the system.
	   kubectl apply command (instead of create and replace command) is intelligent enough to create an object if it doesn't already exist.
	   
      If there are multiple files of objects in a directory, so you can specify path of directory, so all the objects are created at once.
	  # kubectl apply -f /path/to/config-files


Tips for exam: Use imperative commands to save time as much as possible for simple tasks.

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.


======================

KUBECTL APPLY COMMAND:
 It consist 3 things - local yaml file, Live object configurations, and Last applied configurations (json format).
  Last applied configuration is present in live object configurations only in the section - 
	metadata:
		annotations:
			kubectl.kubernetes.io/last-applied-configuration:       
  When we run kubectl apply command, apply command compares local file, live object configuration and last applied configuration stored within the live object configurations file for deciding what changes are to be made.
   kubectl create and replace command do not store last applied configuration like this, so please do not mixup imperative and declarative approach.

=======================

SCHEDULER:

How scheduling works?
Ans: Every pod has field called nodeName which by default is blank.
     The scheduler look for the pod which do not have that property (nodeName) set.
	 Then sheduler schedule the pod on the node by setting the nodeName property to the name of the node by creating binding object.
	 
	 
Suppose if there no scheduler to schedule pods what will happend?
Ans: Pods will be in pending state. But we can manually assign pod to nodes by setting nodeName property in pod definition file. We can assign the pod to node at creation time only using this method.

If I want to assign existing pod to another node?
Ans: Create a binding object and send a POST request.


To schedule pod without scheduler, if your pod is in pending state then you need to delete and recreate it.

Labels & Selectors:
We can specify labels in pod definition file in metadata section.

Service and replicaset using selectors for labels assigned to the pod.

=======================================================

TAINTS AND TOLERATIONS:

Taints are set on node and tolerations are set on pod.

It is nothing to do with security of the cluster. Instead it decides what pods schedule on what nodes.
 
If we have application requirements like with GPU or higher CPU or higher memory, etc. Suppose node01 is created for such application.
First we can restrict the node by applying taint on the node so that other pods can not be place in the node01.
By default pods have no tolerations. So none of the pod will place in node01.
Now to place certain pod (eg. Pod D) on the node we can add tolerance.

But Pod D can be place on any other nodes too. So how to tackle that situation?
Ans: Taints and tolerations only restricting node to accept certain pods. But it does not guarntee that pod D cant be place on other node, it can be.
     If requirement is to place certain pods on particular node, it can be achieve through concept called 'node affinity'.

Taints and tolerations does not ask pod to go on particular node, instead it asks node to accept only particular pod which has certain tolerations.

How to taint a node? 
Ans: kubectl taint nodes node-name key=value:taint-effect

taint-effect :- It will tell what happens to Pods if they do not tolerate the taint.
                There are three taint-effects:
				1) NoSchedule         :- Pods will not be schedule on the node if they do not tolerate the taint
				2) PreferNoSchedule   :- System will try to avoid placing pod if they do not tolerate the taint on the node but that is not guaranteed.
				3) NoExecute          :- New pod will not be schedule on the node, and existing pod will be evicted if they do not tolerate the taint. These pod may have been scheduled on the node before the taint was applied on the node.

kubectl taint node node1 app=blue:NoSchedule

Why scheduler does not schedule any pod on master node?
Ans: This is because by default master node is tainted automatically in the cluster that prevents any pod from being scheduled on this node.
     We can modify this behaviour if required. But the best practise is not to deploy application workloads on a master server.\
	 To see the taints run - # kubectl describe node master | grep Taint
	 
How to remove taint from the master node?
Ans: kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-


=========================================================

NODE SELECTORS:

Suppose we have three nodes node1, node2 and node3 from which node1 is larger capacity node. And I want to scheudle my pod on node1.
First add label to the node1 using command - 
# kuebctl label nodes <node-name> <label-key>=<label-value>
# kuebctl label nodes node1 size=Large 

Now in pod definition file add node selector section in spec:
nodeSelector:
	size: Large

But limitations of node selectors are - 
1. We can use single label and selector to achieve our goal.
2. If our requirement is place a pod on node which has large or medium size label 
3. Place a pod on node which is not small
Above we cannot achive using node selector, as we cannot provide advance expressions like OR, NOT. So here 'node affinity' and 'Antiaffinity' features comes into picture.


===================================================


NODE AFFINITY:

Primary feature of node affinity is to make sure pod is palce on particular node. Syntax is complex. We can achive above node slector scenario here with code in spec:

affinity:
	nodeAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
			nodeSelectorTerms:
			- matchExpressions:
			  - key: size
			    operator: In
				values:
				- Large


If our requirement is place a pod on node which has large or medium size label 
affinity:
	nodeAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
			nodeSelectorTerms:
			- matchExpressions:
			  - key: size
			    operator: In
				values:
				- Large
				- Medium

Place a pod on node which is not small
affinity:
	nodeAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
			nodeSelectorTerms:
			- matchExpressions:
			  - key: size
			    operator: NotIn
				values:
				- Small


If we have not set label Small to the node, above example can also write as -
affinity:
	nodeAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
			nodeSelectorTerms:
			- matchExpressions:
			  - key: size
			    operator: Exists
				

What if someone changes the labels of the node? What will happened to the pod?
Ans: This conditions defines in long property in nodeAffinity also known as type of nodeaffinity.
      requiredDuringSchedulingIgnoredDuringExecution
	  preferredDuringSchedulingIgnoredDuringExecution
	  requiredDuringSchedulingRequiredDuringExecution
	  
	  

  During Scheduling -> It is a state where pod does not exist and is created for the first time.
  What if nodes with matching labels (of pod) not available?
  Ans: Eg. we forgot to label the node as large. Here type of node affinity comes into picture.
  
  requiredDuringScheduling --> Scheduler will mandate that pod is place on the node with a given affinity rules. If it cannot find one, then pod will not be schedule.
								This can be useful where placement of the pod is crucial. If matching node does not exist then pod will not be schedule.
  preferredDuringScheduling --> If pod placement is less important than running the workload itself, preferredDuringScheduling will be in use.
								If matching nodes not found then scheduler ignore node affinity rules and placed the pod on any available node.

  DuringExecution :-> It is a state where pod is already running and change is made in the environment (changing/removing label of the node) that affects node affinity.
  What will happened to the pod that are running on the node?
  Ans: 
	IgnoredDuringExecution --> Pod will continue to run if labels removed from the node
	RequiredDuringExecution --> If labels removed, it will evict/remove pods which are running on the node which do not meet affinity rules.
  

Examples:

1)
controlplane ~ ➜  cat red.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                 - key: node-role.kubernetes.io/control-plane
                   operator: Exists
 
 2)
controlplane ~ ➜  cat blue.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: color
                  operator: In
                  values: 
                    - blue
      containers:
      - image: nginx
        name: nginx
        resources: {}

===================================================================================


Scenario: I want to schedule particular pods on particular nodes only and need to make sure those nodes does not accepts other pods as well.
Ans: We can use Taints and Tolerations and Node Affinity together to achieve.

================================================================================

RESOURCES and LIMITS:

By default kubernetes does not set any resource limits for running containers.
When scheduler schedule the pod on the node it takes into consideration that amount of resources required by the pod to run and whether those available on the node or not.
If no resources available on any node, then scheduler hold back the pod and we can see pod is in pending state, and we can see 'Insufficient cpu' error when run - # kubectl describe pod nginx command


Resource Request --> We can specify amount of CPU and Memory required by the pod when it runs. Eg: 1 CPU and 1Gi Memory. So when scheduler tries to schedule the pod it will check whether this amount of resources available on the node or not.
                     spec:
						containers:
						- name: nginx-container
						  image: nginx
						  ports:
							- containerPort: 8080
						  resources:
							requests:
							  memory: "100Mi"
							  cpu: 0.5
					After deploying, we can see resource requests in # kubectl describe pod nginx command 

                 CPU values can be set from 0.1 which is also can be set as 100m, m -> milli. 1 CPU means 1 GCP Core.
				 Memory values can be set from 256Mi.
		
		-----------------------------------------
		1 G (Gigabyte) = 1,000,000,000 bytes
		1 M (Megabyte) = 1,000,000 bytes
		1 K (Kilobyte) = 1,000 bytes
		
		1 Gi (Gibibyte) = 1,073,741,824 bytes
		1 Mi (Mebibyte) = 1,048,576 bytes
		1 Ki (Kibibyte) = 1,024 bytes
		-----------------------------------------


Resource Limit --> By default container has no limit to cosume resource from the node. So it can be possible sometimes one container can occupie all resources from the node, hence it prevents other pod to be schedule on the node.
				   We can specify the limit for memory and cpu for the container so it cannot consume more than that resources from the node.
				   Limits and requests are set for each container in the pod, if the pod is multi-container then each container can get specified amount of request and limit.
				   
					

Difference Request vs Limit:
Request -> Scheduler will check if mentioned amount of cpu, memory present in the node before placing the pod.
Limit -> Container of pod cannot consume resources beyond this limit.


What happens when pod tries to exceed limits (CPU/Memory) which is mentioned?
Ans: A container cannot use more CPU than its limit.
     In case of memory, a container can use more memory than its limit. If pod tries to consume  more memory than its limit constantly then pod will be terminated with an OOM (Out Of Memory) error in the logs in describe command.
	 
If I specified limits block only and not requests, what will happen?
Ans: Kubernetes automatically sets requests same as limit.

Most ideal setup is set requests and no limits for CPU- 
   Container A - 1vCPU and Container B - 1vCPU. Now if container A wants more CPU and container B not using that much CPU then container A can consume.
   When container B requires more CPU cycle, then it will be guranteed its CPU requested cycle.
   For this approach we need to make sure that we need to set resource request for each pod, then only pod will get guranateed CPU.


 For memory if we set Requests and No limit - If container A consume all the memory and now container B wants more memory, only option is to kill container A. Because unlike CPU, we cannot throttle memory.
                                              Once memory is assigned to a pod, only way to free up memory is to kill a pod.


Limit Range :- 

How we ensures that every pod has created has some default limit set?
Ans: This is possible with limit ranges. Limit ranges help us to define default values to be set for containers in a Pods which are created without specifying limit in pod-definition files. 
     This is applicable at the namespace level.

When we create limit range, it will affect newer pods which are going to be created and it does not affect existing pods.


=========================================

RESOURCE QUOTA:
Resource limits set on container level and Resource quota sets on namespace level.
If we want to restrict all pods should consume this much amount of cpu/memory then we can use resource quota.
Resourced quota is a namespace level object that can be created to set hard limits for requests and limits.


Document reference:
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/

============================================


A quick note on editing Pods and Deployments
Edit a POD
Remember, you CANNOT edit specifications of an existing POD other than the below.

spec.containers[*].image

spec.initContainers[*].image

spec.activeDeadlineSeconds

spec.tolerations

For example you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:

1. Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.

A copy of the file with your changes is saved in a temporary location as shown above.

You can then delete the existing pod by running the command:

kubectl delete pod webapp



Then create a new pod with your changes using the temporary file

kubectl create -f /tmp/kubectl-edit-ccvrq.yaml



2. The second option is to extract the pod definition in YAML format to a file using the command

kubectl get pod webapp -o yaml > my-new-pod.yaml

Then make the changes to the exported file using an editor (vi editor). Save the changes

vi my-new-pod.yaml

Then delete the existing pod

kubectl delete pod webapp

Then create a new pod with the edited file

kubectl create -f my-new-pod.yaml



Edit Deployments
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command

kubectl edit deployment my-deployment

===============================================================================================
Run: #kubectl describe pod nginx
Error out of memory: Back-off restarting failed container mem-stress in pod elephant_default
Look into the section in Containers.Last State: Reason: OOMKilled
===============================================================================================

DAEMON SETS:
It ensures that one copy of pod is always presnt in all nodes in the cluster.
Whenever new node is added to the cluster a replica of pod is automatically added to that node, and when node is removed, the pod is automatically removed.

Use cases:
	Monitoring solution - If we want to deploy monitoring agent on each node 
	Logs Viewer - If we want to deploy logs collector on each node 

In a cluster kube-proxy component is deployed as daemonset in a cluster.
Also, one more example is deploying netwoking solution like calico.

Daemon Set definition file exactly like Replica set except kind: DaemonSet


How does dameonset work?
Ans: Till version v1.12 daemon set was using nodeName property to schedule pod on each node.
	 From v1.12 daemon set uses NodeAffinity and default scheduler.

Tip: In exam if we want to create DaemonSet, create deployment using command with dry-run option and then change it to DaemonSet, this will be quick as compared to Kubernetes documentation search.
===============================================================================================

STATIC POD :

The pods that are created by the kubelete on its own without the help of kube-api server or rest of the kubernetes cluster componenets are known as static pod. 

In kubernetes cluster, kubelets schedule a pod based on instruction receives from kube-apiserver, which is decides by kube-scheduler and stores in etcd database.
What if there is no master at all? Can we still able to schedule a pod on a particular node?
Ans: Yes, we can. A kubelet can manage the node independently.
     But how we can provide a pod-definition file to kubelet without kube-apiserver.
	 We can configured a kubelet to read the Pod definition files from a directory on a server designated to store information about Pods. eg: /etc/kubernetes/manifests
	 kubelet periodically checks this directory for files, reads this files and create a pods on the host. It also ensures that pod stays alive. If application crashes then kubelet attempts to restart the pod.
	 If we make change to any of the pod file in the directory then kubelet recreates the pod for those changes to take effect.
	 If we remove the file from this directory, the pod is automatically deleted.
	 
Please note we can only create pods this way, we cannot create deployments, replicaset or services by placing a definition file in a designated directory.

Kubelet works on pod level and only understand pods that is why it is able to create pods this way.

Two options to ask kubelet to create pods:
1) The custom folder path is needs to set in kubelet service file with option --pod-manifest-path=/etc/kubernetes/manifests and restart the service.
2) Instead of mentioning path directly in kubelet service, we can add --config=kubeconfig-1.yaml file and add staticPodPath:/etc/kubernetes/manifests there.
 
	 
Is API server aware about the static pod is created?
Ans: yes it is. we can check using #kubectl get pods command in master node. But we cannot edit or delete it like normal pod. We can only delete them by modifing the file from manifests folder on the node.

Note: static pod name is automatically appended by node name. eg: nginx-worker1

Use case:
We can use static pods to deploy control plane components. We can first install kubelet and then creates pod definition files that uses docker images of various control plane components such as API server, etcd, kube-scheduler, etc. Place files in manifests folder and then kubelet will deploy controlplane components themselves.
   This way we don't have to download binaries, configure services, or worry about services crashing. If any of the service crash, since it is static pod kubelet will automatically recreates the pod.
   
Kubeadm tool setup the cluster using static pod mechanism only.


Difference between Static Pods and Daemonset.
Ans: 
Static Pods --> Created by kubelet.
				Use to deploy control plane component as a static pod.
				It is ignored by kube-scheduler.

Daemon Sets --> Created by kube-apiserver.
				Use to deploy monitoring agents, logging agents on node.
				It is also ignored by kube-scheduler.

How to identify static pods?
Ans: Two ways:
     1) Looking at pod name. In the pod name node name is appended. #kubectl get pods
	 2) kubectl get pods nginx -o yaml
	    Look for the ownerReferences section and under this kind: Node 
		 if it is Node that means it is static pod.

============================================================================================

MULITIPLE SCHEDULERS:

A kubernetes cluster can have multiple scheduler at a time. We can use our custom scheduler instead of default scheduler to deploy pods.

If you have specific application that requires its component placed on nodes after doing some additional checks. We can decide our own algorithm instead of default one to place pods on node. For this we can write our own kubernetes scheduler program, package it and deploy it as the default scheduler or additional scheduler in kubernetes cluster.

Provide different names to the different scheduler by configuring in kube-scheduler configuration file.

---
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler 

Deploy Additional Scheduler:
1) Download binary
2) Update service
   my-scheduler.service
   ExecStart=/usr/local/bin/kube-scheduler \
		--config=/etc/kubernetes/config/kube-scheduler.yaml

Create separate configuration file for each scheduler.


Deploy Scheduler as a Pod:
---
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml

    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    name: kube-scheduler
	
We can also specify leader elect option when we have multiple scheduler on different master node as high-availability setup, only one scheduler can be active at a time, so we can use leader elect option to decide who leads scheduling activity.

Study how to deploy multiple scheduler using deployment option.

Once you deploy scheduler, how to use it?
Ans: Add schedulerName field in spec section of pod and specify name of the scheduler like -
		schedulerName: my-custom-scheduler
	 No need to specify scheduler namepsace.

If scheduler not configured correctly then pod will be in pending state.


How we know which scheduler picked it up by Pod as we have multiple scheduler?
Ans: We can view this in the events:
		# kubectl get events -o wide

How to view the logs of scheduler?
Ans: # kubectl logs my-custom-scheduler -n kube-system 
        where my-custom-scheduler is a pod name or deployment name


============================================================================================
How sheduler deployes/ schedule pod on the node?
Ans: 1) Scheduling Queue --> Pods are wait to be scheduled, and sorting happens based on pods with higher priority 
	 2) Filtering --> Nodes that cannot have sufficient resources and cannot run the pods filters out
	 3) Scoring --> Nodes are scored with different weights based on free space that will have after reserving the CPU required for that pod.
     4) Binding --> Pod is binds to a node with highest score.

 All above operations are achived with certain plaugins.
 Like 	Scheduling Queue -- PrioritySort plugins (Sets the priority of the pod to be scheduled based on priority class)
		Filtering -- NodeResourceFit (Check right node)
		             NodeName (Check whether nodeName field is specified in the pod if yes it filters other nodes)
					 NodeUnschedulable (Check whether nodeunschedulable field is set, this is when node is corden)
		Scoring --  NodeResourceFit (How much space will free of node after pod is placed)
					ImageLocality  (High Schore with node which already has container image)
		Binding -- DefaultBinder
		
We can also customized what plugins go where using 'Extension Points'.
       Scheduling Queue -- queueSort
	   Filtering -- preFilter, filter, postFilter
	   Scoring -- preScore, score, reserve
	   Binding -- permit,  preBind, bind, postBind
	   
How we can change default behaviour of pluggins are called and how we can get our own plugins?
Ans: 

v1.18 -> We can set multiple profiles in same scheduler functionality introduced.		
			Benefits of this we do not need to run separate binaries of multiple scheduler, instead multiple scheduler can share same binary.

my-scheduler-2-config.yaml
apiversion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
	- schedulerName: my-scheduler-2
	- schedulerName: my-scheduler-3
	- schedulerName: my-scheduler-4

How to configure above different scheduler profiles work differently, because for now it works like default scheduler?
Ans: Under each scheduler profile we can configure plugins which we want to.
	my-scheduler-2-config.yaml
	apiversion: kubescheduler.config.k8s.io/v1
	kind: KubeSchedulerConfiguration
	profiles:
		- schedulerName: my-scheduler-2
		  plugins:
			score:
				disabled:
					- name: TaintToleration
				enabled:
					- name: MyCustomPluginA
					- name: MyCustomPluginB
		- schedulerName: my-scheduler-3
		  plugins:
			preScore:
				disabled:
					- name: '*'
			score:
				disabled:
					- name: '*'
		- schedulerName: my-scheduler-4

============================================================================================

MONITORING:

What are things to monitor?
Ans: How many nodes are healthy, node performance metrix such as CPU, Memory, Network, and Disk utilization. 
	 Pod level matrix such as no. of pods and performance metrix of each pod such as CPU, Memory consumption.

For now kubernetes does not comes with full-featured built-in monitoring solution. 
	We can relie on open source solution such as -
		Metrics Server, Prometheus, Elastic Stack
	Proprietary solutions like -
		Datadog and Dynatrace

Metrics Server -> It is in memory monitoring solution and does not stores the metrics on disk. So we cannot see historical data.

How kubernetes generates metrics?
Ans: We know that kubelet present in each node. Kubelet contains the sub-component known as cAdvisor or Container Advisor.
	cAdvisor is responsible for retriving performance metrics from pod and exposing them through the kubelete API to make metrics available for the Metrics Server.
	

Metrics Server:-
How to deploy?
Ans: For minikube: # minikube addons enable metrics-server
	 For others: # kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

How to check metrics?
Ans: # kubectl top node -> CPU and Memory consumption of each node 
	 # kubectl top pod -> CPU and Memory consumption of each pod 

============================================================================================

LOGGING:

Once the pod is running we can view the logs of pod using: 
	# kubectl logs nginx-pod
	# kubectl logs -f nginx-pod (Stream live logs)

In case of multi-container, we must specify name of container in command.
	# kubectl logs -f nginx-pod nginx-container 
	
============================================================================================

Rollout and Versioning:
When we first create a deployment it triggers a rollout. A new rollout creates a new deployment revision.
In future when application is upgraded, a new rollout is triggered and new deployment revision is created.
This help us to rollback to previous version when needed.

How to check status of rollout?
Ans: # kubectl rollout status deployment/myapp-deployment


To see the revision and history of rollout :-
# kubectl rollout history deployment/myapp-deployment 



Types of Deployment Startegy:
1) Recreate
2) Rolling updates (Default strategy)
	
Recreate -> Destroy all pods and create new one
Rolling Updates -> Take down older version and bring down newer version one by one. Hence application does not have downtime.
                    

When we update deployment property like updating image, labels and when we run kubectl apply command:-
 New rollout is triggered and a new revision of the deployment is created 
 
We can also use below command to update the image of the application :-
# kubectl set image deployment/myapp-deployment nginx-container=nginx:1.9.1
but in this way deployment file is not updated so be careful with that.


To check deployment strategy is recreate or rolling update, you can use -
	# kubectl describe deployment myapp-deployment 
   and check StrategyType section.
   Also check events here you can see if strategy is Recreate then scale down of replicaset to zero first.
    and If strategy is RollingUpdate then old replicaset scale down one at a time, simultaneously scale up new replicaset one at a time.

	
When we create deployment it first creates the replicaset then pods under that. So when we do upgrades, first it creates new replicaset and then creates pod under that.
You can check both replicaset using command: # kubectl get replicaset 
When we rollback using -
	# kuebctl rollout undo deployment/myapp-deployment 
  deployment destroys the pod in the new replicaset and bring older ones up in old replica set.
 


Sample code with Recreate startegy in deployment:

spec:
    strategy:
        type: Recreate

------------

Sample code with Rolling Update startegy in deployment:

spec:
    strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
                    



controlplane ~ ➜  cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done


============================================================================================

Commands and Argument:

command filed in pod is same as ENTRYPOINT in dockerfile
args in pod is same as CMD in dockerfile.

============================================================================================

Enviornment Variables:

We can set environment varibles in pod in spec.containers.env property

env:
	- name: APP_COLOR
	  value: red

============================================================================================
ConfigMap:

When we have lots of pod-definition files it is difficult to manage env data directly in pod definition files.
In such case we can create configmap with the value and pass configmap to the pod definition file.

Config map is used to pass configuration data in the form of key-value pairs in kubernetes.

We can create configmap using imperative way or declarative way.

Imperative way:
# kubectl create configmap <config-name> --from-literal=<key>=<value> --from-literal=<key>=<value>


# kubectl create configmap <config-name> --from-file=<path-to-file>


Declarative way:

---
apiVersion: v1
kind: ConfigMap
metadata:
    name: app-config
data:
    APP_COLOR: blue
    APP_MODE: prod



View configmap:
# kubectl get configmaps
# kubectl describe configmaps

Use config map in pod:
1) In pod definition file: spec.containers.envFrom

envFrom:
- configMapRef:
	name: app-config

2) Single env variable:
env:
	- name: APP_COLOR
	  valueFrom:
		configMapKeyRef:
			name: app-config
			key: APP_COLOR
			
3) As a volume:
volumes:
- name: app-config-volume
  configMap:
	name: app-config

============================================================================================

Secrets:

Why secrets required? 
Ans: If I want to pass some sensitive data like password, defining variable value in pod definition file directly and configmap is not a good option as it stores values in plane text.
      So here we can use Secrets. Secrets stores information in encoded format. 
	  
We can create secrets in imperative way and declarative way.

Imperative:
# kubectl create secret generic <secret-name> --from-literal=<key>=<value>
# kubectl create secret generic <secret-name> --from-file=<path-to-file>

# kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd
# kuebctl create secret generic app-secret --from-file=app_secret.properties

When we run a imperative command, the secret values are automatically stored in base64 encoded format in secrets.


Declarative:
---
apiVersion: v1
kind: Secret
metadata:
    name: app-secret
data:
    DB_Host: mysql
    DB_User: root
    DB_Password: paswrd
	
But we have provided secret values in plain text which is not good way, so we must provide values in an encoded format.

How to convert data from plain text to encoded format?
Ans: Using base64
     # echo -n 'mysql' | base64
	 bXlzcWw=
	 

Type of secrets:
A generic type secret indicate an Opaque secret type.
A docker-registry type secret is for accessing a container registry.
A tls type secret holds TLS certificate and its associated key.


To get more information about secret we created run:
	# kubectl describe secret app-secret 
  It hides the value of secret.
  
To get the values of secret:
	# kubectl get secret app-secret -o yaml
   No we can see values as well, but it is in encoded format. 
 
To decode values of secret:
	# echo -n 'bXlzcWw=' | base64 --decode 

How to use Secrets in pods?
Ans: 
Injecting secrets as env variable in a pod spec.containers.envFrom :
	  envFrom:
      - secretRef:
          name: app-secret

As a single env:
      env:
      - name: DB_Password
        valueFrom:
            SecretKeyRef:
               name: app-secret
               key: DB_Password

As a volume:
      volumes:
      - name: app-secret-volume
        secret:
          secretName: app-secret			   

When we inject secret as a volume in a pod, each attribute  in the secret is created as a file with the value of the secret as its content.
	# ls /opt/app-secret-volume
	 DB_Host   DB_Password   DB_User

Note: Secrets are not Encrypted. Only encoded.
      Secrets are not encrypted in ETCD (encryption at rest in not present by default).
	  By default anyone able to create pods/deployments in the same namespace where secrets resides can access the secrets. So recommendation is configure least-privilege access to Secrets - RBAC
	  Consider third-party secrets store providers like AWS, GCP, Azure, Vault provider

Notes:

Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 



Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

============================================================================================


MULTI-CONTAINER POD:
There can be requirement where two services needs to work together, such as web server and logging service.
We need one web server instance with one log agent instance paired together and don't want to merge and load the code of the tool services together, but still want to work together.
Also we want that those instances can scale up and down together. Here multi-container pods comes into picture.

They share same lifecycle, same network space, they can access to same storage volumes.

There are 3 common patterns, when it comes to designing multi-container PODs. The first and what we just saw with the logging service example is known as a side car pattern. The others are the adapter and the ambassador pattern.

=============================================================================================

InitContainers:
In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.

But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts. That's where initContainers comes in.

An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section,  like this:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']

	
When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. 

You can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case each init container is run one at a time in sequential order.

If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

In pod get command: # kubectl get pod  :when you see Status 'Init:CrashLoopBackOff' it means init container is failing.

=============================================================================================================================
SELF HEALING APPLICATIONS

Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes.
=============================================================================================================================

CLUSTER MAINTENANCE:


When nodes go down, pods on them are inaccessible.
If node came back immediately, a kubelet process starts anbd the pod come back online.
If node was down for more than 5min, the pods are terminated from that node.
If pods are part of replicaset then they are recreated on other nodes, and if pods are standalone then it dead.
The time it waits for pod to come back online is known as pod-eviction-timeout. It sets on controller manager: kube-controller-manager --pod-eviction-timeout=5m0s
When node comes online after pod-eviction-timeout, it comes up blank without any pod scheduled on it.

How to perform OS upgarde on node?
Ans:
We can purposefuly drain the node of all workload, so that workloads are moved to other nodes in the cluster.
	# kubectl drain node-1
When we drain the node, pods are gracefully terminated and recreated on other node.
The node is also cordoned or marked as unschedulable automatically when we drain the node. Meaning no pods can be schedule on this node until we specifically removes restriction.
Now perform os patching - 
	# yum update -y
	# reboot
When nodes comes back online it is still unschedulable, so we need to uncordon it so that pods can be schedule.
    # kubectl uncordon node-1

Note that the pods which are moved on other nodes dont automatically fall back. If any of these pods are deleted or if new pods were created on cluster they can be schedule on this node.


There is one more command:
	# kubectl cordon node-2
 It marks node unschedulable, so new pods not deploy on this node. 
 Unlike drain it does not terminate or moves the pod different node.


When we cordon node and check # kubectl describe node worker1
You can see Unschedulable filed is set to true.
Also in #kubectl get nodes command, in Status field it is showing 'SchedulingDisabled'.	

If we try to drain node and pod is present which is not part of replicaset we will get an error:
	'cannot delete cannot delete Pods that declare no controller (use --force to override)'
 now to drain it run command:
	# kubectl drain worker --ignore-daemonsets --force 
	
=============================================================================================================================

KUBERNETES VERSION:

v1.11.3:
v1 -> Major Version 
11 -> Minor Version  (Released every few months with new features and functionality)
3  -> Patch Version  (Released more often with critical bug fixes)

There are other relases called alpha and beta.
All bug fixes and improvements first go into an alpha release. eg: v1.11.4-alpha In this release the features are disabled by default and may be buggy.
Code is well tested in beta release. eg: v1.11.4-beta. New features are enabled by default.
After beta they merged into main stable version. Eg: v1.11.4

When we download package and extract of new version, it will has most of the component of same version except etcd, coreDNS, etc. as they are separate projects.
Eg: Package - v1.13.4 
      kube-apiserver - v1.13.4
	  controller-manager - v1.13.4
	  kubelet - v1.13.4
	  kube-scheduler - v1.13.4
	  kube-proxy - v1.13.4
	  kubectl - v1.13.4
	  ETCD Cluster - v3.2.18
	  CoreDNS: v1.1.3

